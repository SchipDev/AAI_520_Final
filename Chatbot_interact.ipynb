{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0sgqyzVCrHYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c97005-a864-436f-ad7a-c42ae58cb2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up\n",
        "\n",
        "Lets start by importing all of our dependenies and specifying a feww key directories."
      ],
      "metadata": {
        "id": "ieCWq5LuPG54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GOL0O-ZOdCsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to dataset csv\n",
        "csv_file_path = '/content/drive/My Drive/trained_models/squad_data.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Set up device to GPU if present and CPU if not\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrRSHixctzIy",
        "outputId": "8a088b25-e395-43cc-a8af-7d9435776966"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lest load our model and tokenizer from the weights we saved from our fine-tuned model."
      ],
      "metadata": {
        "id": "naVt6GZxP_II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in model and tokenizer\n",
        "saved_model_path = '/content/drive/My Drive/trained_models/trained_model_bert_csv'\n",
        "tokenizer = BertTokenizer.from_pretrained(saved_model_path)\n",
        "model = BertForQuestionAnswering.from_pretrained(saved_model_path)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aymUmsbht2u4",
        "outputId": "3d6b7d91-7307-46bc-e2bb-9076f5b5c02e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to define a function to assist in finding the models most optimal output to the users prompt."
      ],
      "metadata": {
        "id": "1lEcQfsZQUp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_answer(query, df):\n",
        "    # Check cache first\n",
        "    if query in cache:\n",
        "        return cache[query]\n",
        "\n",
        "    # Initialize variables to store the best answer and its score\n",
        "    best_answer = None\n",
        "    best_score = float('-inf')\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        context = row['Context']\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(query, context, return_tensors='pt', max_length=512, padding=True, truncation=True, add_special_tokens=True)\n",
        "\n",
        "        # Get model output\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs.to(device))\n",
        "\n",
        "        # Get start and end scores\n",
        "        start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "        # Get the best start and end positions for this context\n",
        "        start_idx = torch.argmax(start_scores)\n",
        "        end_idx = torch.argmax(end_scores)\n",
        "\n",
        "        # Calculate the total score for this answer\n",
        "        score = start_scores[0, start_idx].item() + end_scores[0, end_idx].item()\n",
        "\n",
        "        # Update the best answer if this answer is better\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "          best_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx+1]))\n",
        "\n",
        "        # Implement early stopping based on a confidence threshold\n",
        "        if best_score > 0.9:  # Assuming 0.9 is the confidence threshold\n",
        "            break\n",
        "\n",
        "        #print(f\"Score: {best_score}, Best_answer: {best_answer}\")\n",
        "\n",
        "    # Store the result in cache\n",
        "    cache[query] = best_answer\n",
        "\n",
        "    return best_answer\n"
      ],
      "metadata": {
        "id": "IC3XVjPFt8SU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define the interaction loop for our model. After this is run, a text input field will pop up below the cell. Enter your prompt into that cell and hit enter to ask the bot. Use the following commands to facillitate your interactions.\n",
        "\n",
        "* Use \"quit\" to exit interaction with the bot\n",
        "* Type \"set context to\" followed by a subject to switch the bots context to another subject area. Some examples are \"Tibet\", \"Chopin\", and \"Beyonce\"."
      ],
      "metadata": {
        "id": "0XF23ctJQqCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cache = {}\n",
        "current_context = \"\"\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"You: \")\n",
        "    if user_query.lower() == \"quit\":\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Set the context if the user specifies it\n",
        "    if \"set context to\" in user_query.lower():\n",
        "        current_context = user_query.lower().replace(\"set context to\", \"\").strip()\n",
        "        print(f\"Context set to: {current_context}\")\n",
        "        continue\n",
        "\n",
        "    # Filter DataFrame based on the current context\n",
        "    if current_context != None:\n",
        "        filtered_df = df.reset_index()\n",
        "        filtered_df = filtered_df.loc[filtered_df['Context'].str.contains(current_context, case=False)]\n",
        "        if filtered_df.empty:\n",
        "            print(\"Bot: Sorry, I don't have information on that context.\")\n",
        "            continue\n",
        "    else:\n",
        "        filtered_df = df.reset_index()\n",
        "\n",
        "    # Find the best answer using BERT\n",
        "    best_answer = find_best_answer(user_query, filtered_df)\n",
        "    print(f\"Bot: {best_answer}\")\n",
        "    #context = ''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYjRAjMXvLJw",
        "outputId": "410cdafe-fd49-4db6-f955-5d21355d0b93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Who was in charge?\n",
            "Bot: mathew knowles\n",
            "You: set context to tibet\n",
            "Context set to: tibet\n",
            "You: Who was in charge\n",
            "Bot: drogon chogyal phagpa\n",
            "You: quit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWOmkUFNImsk"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}